{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA Models\n",
    "Done by: Baga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bagautdinnukhkadiev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "documents = pd.read_csv('data/documents_subset.csv', sep='\\t')\n",
    "documents['doc_title'] = documents['doc_title'].replace(r'[^\\w\\s]',' ',regex=True).replace(r'\\s+',' ',regex=True).str.lower()\n",
    "queries = pd.read_csv('data/queries.csv', sep='\\t')\n",
    "queries['query_text_rus'] = queries['query_text_rus'].replace(r'[^\\w\\s]',' ',regex=True).replace(r'\\s+',' ',regex=True).str.lower()\n",
    "\n",
    "import spacy\n",
    "from spacy import load\n",
    "from spacy.lang.ru.examples import sentences\n",
    "from spacy.lang.ru import Russian\n",
    "\n",
    "\n",
    "nlp = Russian()\n",
    "load_model = load(\"ru_core_news_sm\")\n",
    "\n",
    "lemma = []\n",
    "\n",
    "for doc in load_model.pipe(documents[\"doc_title\"].values.astype(str).tolist()):\n",
    "    lemma.append([n.lemma_ for n in doc])\n",
    "\n",
    "\n",
    "# lemma = [' '.join(i) for i in lemma]\n",
    "documents['doc_title_clean'] = lemma\n",
    "lemma = []\n",
    "\n",
    "for doc in load_model.pipe(queries[\"query_text_rus\"].values.astype(str).tolist()):\n",
    "    lemma.append([n.lemma_ for n in doc])\n",
    "\n",
    "\n",
    "# lemma = [' '.join(i) for i in lemma]\n",
    "queries['query_text_clean'] = lemma\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "documents['doc_title_clean'] = documents['doc_title_clean'].apply(lambda x: [item for item in x if item not in stopwords_ru])\n",
    "documents['doc_title_clean_as_str'] = [' '.join(map(str, l)) for l in documents['doc_title_clean']]\n",
    "# documents['doc_title_clean_as_str']\n",
    "\n",
    "\n",
    "queries['query_text_clean'] = queries['query_text_clean'].apply(lambda x: [item for item in x if item not in stopwords_ru])\n",
    "queries['query_text_clean_as_str'] = [' '.join(map(str, l)) for l in queries['query_text_clean']]\n",
    "# queries['query_text_clean_as_str']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create TF-IDF Term-Document Matrix\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "term_document_matrix = tfidf_vectorizer.fit_transform(documents['doc_title_clean_as_str'].values.astype('U'))\n",
    "# term_document_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np \n",
    "\n",
    "# Step 2: Apply SVD\n",
    "n_components = 500  # Set number of latent dimensions\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "svd_matrix = svd.fit_transform(term_document_matrix)\n",
    "\n",
    "# Extract U, Sigma, V^T\n",
    "U = svd.components_.T  # Term-topic matrix\n",
    "Sigma = np.diag(svd.singular_values_)  # Diagonal matrix of singular values\n",
    "VT = svd.components_  # Document-topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to project query into topic space\n",
    "def compute_query_vector_projection(query, tfidf_vectorizer, U):\n",
    "    # Transform the query into a sparse vector using the same TF-IDF vectorizer\n",
    "    sparse_query_vector = tfidf_vectorizer.transform([query]).toarray()[0]  # q is mx1\n",
    "\n",
    "    # Project sparse query vector into the topic space\n",
    "    query_vector_topic_space = np.dot(U.T, sparse_query_vector)  # q' = U^T q\n",
    "    return query_vector_topic_space\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def compute_cosine_similarity(query_vector, document_vectors):\n",
    "    # Normalize query and document vectors\n",
    "    query_norm = query_vector / np.linalg.norm(query_vector)\n",
    "    doc_norms = document_vectors / np.linalg.norm(document_vectors, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarities = np.dot(doc_norms, query_norm)\n",
    "    return cosine_similarities\n",
    "\n",
    "# Compute rankings for all queries\n",
    "rankings = {}\n",
    "for _, row in queries.iterrows():\n",
    "    query_id = str(row['query_id'])  # Convert query_id to string for consistency\n",
    "    query_text = row['query_text_clean_as_str']\n",
    "    \n",
    "    # Project query into topic space\n",
    "    query_vector_projected = compute_query_vector_projection(query_text, tfidf_vectorizer, U)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarities = compute_cosine_similarity(query_vector_projected, VT.T)  # VT.T is NxK\n",
    "    \n",
    "    # Rank documents and get scores\n",
    "    ranked_docs_indices = np.argsort(-cosine_similarities)  # Sort descending\n",
    "    ranked_docs = [\n",
    "        (documents.iloc[i]['doc_id'], cosine_similarities[i]) for i in ranked_docs_indices\n",
    "    ]\n",
    "    \n",
    "    # Store the results in the rankings dictionary\n",
    "    rankings[query_id] = ranked_docs\n",
    "\n",
    "# Print the rankings dictionary\n",
    "# print(\"Rankings Dictionary:\")\n",
    "# for query_id, results in rankings.items():\n",
    "#     print(f\"Query ID {query_id}: {results[:5]}\")  # Display the top 5 results for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import nDCG, MAP, RBP, Recall, Qrel, ScoredDoc\n",
    "\n",
    "\n",
    "qrels_pd = pd.read_csv('data/qrels.csv', sep='\\t')\n",
    "\n",
    "qrels = [\n",
    "    ir_measures.Qrel(query_id=str(row['query_id']), doc_id=row['doc_id'], relevance=row['relevance_class'])\n",
    "    for _, row in qrels_pd.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>ecd7c3e5-b990-4ae5-bf62-a65964f7d7ca</td>\n",
       "      <td>0.602216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>91840fdf-31a7-40ca-86db-7e9c9ee26d24</td>\n",
       "      <td>0.577852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>3c9b2cbb-a4b2-491c-94f9-e47099c0f75e</td>\n",
       "      <td>0.547494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>ca3c625d-a4ff-4915-89a1-87e892b8e310</td>\n",
       "      <td>0.537686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>ba7080d3-d795-47f6-85b0-4ceb5799738e</td>\n",
       "      <td>0.537438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>463bdca0-dd6c-40f4-8c56-cef44ab58cc4</td>\n",
       "      <td>0.527697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200</td>\n",
       "      <td>6343b421-e1ce-45cb-85bf-4b7900c49fdf</td>\n",
       "      <td>0.527697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>784757d8-4410-4b4a-83e3-7f94bc284a50</td>\n",
       "      <td>0.522468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200</td>\n",
       "      <td>b77c3fa1-7879-427b-b482-d80afad036e7</td>\n",
       "      <td>0.522257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200</td>\n",
       "      <td>9b5f1c6e-aa46-4a0d-b403-fb29f26fa820</td>\n",
       "      <td>0.461142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                doc_id     score\n",
       "0      200  ecd7c3e5-b990-4ae5-bf62-a65964f7d7ca  0.602216\n",
       "1      200  91840fdf-31a7-40ca-86db-7e9c9ee26d24  0.577852\n",
       "2      200  3c9b2cbb-a4b2-491c-94f9-e47099c0f75e  0.547494\n",
       "3      200  ca3c625d-a4ff-4915-89a1-87e892b8e310  0.537686\n",
       "4      200  ba7080d3-d795-47f6-85b0-4ceb5799738e  0.537438\n",
       "5      200  463bdca0-dd6c-40f4-8c56-cef44ab58cc4  0.527697\n",
       "6      200  6343b421-e1ce-45cb-85bf-4b7900c49fdf  0.527697\n",
       "7      200  784757d8-4410-4b4a-83e3-7f94bc284a50  0.522468\n",
       "8      200  b77c3fa1-7879-427b-b482-d80afad036e7  0.522257\n",
       "9      200  9b5f1c6e-aa46-4a0d-b403-fb29f26fa820  0.461142"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten rankings into a DataFrame\n",
    "flattened_rankings = []\n",
    "for query_id, docs in rankings.items():\n",
    "    for doc_id, score in docs:\n",
    "        flattened_rankings.append({'query_id': str(query_id), 'doc_id': doc_id, 'score': score})\n",
    "\n",
    "# Convert to a DataFrame\n",
    "flattened_rankings_df = pd.DataFrame(flattened_rankings)\n",
    "# Ensure documents are sorted by score for each query\n",
    "flattened_rankings_df = flattened_rankings_df.sort_values(by=['query_id', 'score'], ascending=[True, False])\n",
    "\n",
    "# Display the flattened rankings\n",
    "flattened_rankings_df[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(qrels, result):\n",
    "    runs = [\n",
    "        ScoredDoc(query_id=row['query_id'], doc_id=row['doc_id'], score=row['score'])\n",
    "        for _, row in result.iterrows()\n",
    "    ]\n",
    "\n",
    "    metrics = [\n",
    "        ir_measures.nDCG @ 20,   # nDCG@20\n",
    "        ir_measures.AP,          # Average Precision\n",
    "        ir_measures.RBP(rel=1),  # Relevance Based Precision\n",
    "        ir_measures.R @ 100,     # Recall@100\n",
    "        ir_measures.R @ 1000     # Recall@1000\n",
    "    ]\n",
    "\n",
    "    scores = ir_measures.calc_aggregate([nDCG@20, MAP, RBP(rel=1), Recall@100, Recall@1000], qrels, runs)\n",
    "    # scores = ir_measures.calc_aggregate([nDCG@20, MAP, Recall@100, Recall@1000], qrels, runs)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics SVD:\n",
      "nDCG@20: 0.0009344739449582717\n",
      "AP: 0.0028777968463369347\n",
      "R@100: 0.004798560642855103\n",
      "R@1000: 0.051033122420217104\n",
      "RBP(rel=1): 0.002689839650097832\n"
     ]
    }
   ],
   "source": [
    "performance_tfidf = evaluate(qrels, flattened_rankings_df)\n",
    "print(\"Evaluation Metrics SVD:\")\n",
    "for metric, value in performance_tfidf.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1024 components\n",
    "AP: 0.002972477146510521\n",
    "nDCG@20: 0.0009344739449582717\n",
    "RBP(rel=1): 0.0026801942405148675\n",
    "R@100: 0.0028890962043401553\n",
    "R@1000: 0.05221304466258818\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
