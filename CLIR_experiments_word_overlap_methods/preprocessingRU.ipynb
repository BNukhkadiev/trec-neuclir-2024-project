{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code, you might need to uncomment the next cell in order to import Russian-language NLP-library Razdel (https://github.com/natasha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sacremoses import MosesTokenizer\n",
    "import spacy\n",
    "from razdel import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import stopwordsiso\n",
    "from stopwordsiso import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will try several tokenizers on a sample Russian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "Илья Сегалович родился в семье советского геофизика В.И.Сегаловича. В физико-математической школе \n",
    "познакомился с будущим сооснователем «Яндекса» Аркадием Воложем. В 1981 г. поступил в Московский \n",
    "геологоразведочный институт им.Орджоникидзе, где когда-то обучалось 3 тыс. студентов.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK tokenizer\n",
    "nltk_tokenized = nltk.word_tokenize(test_text, language='russian')\n",
    "\n",
    "tokenized['NLTK'] = nltk_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moses tokenizer\n",
    "mt = MosesTokenizer(lang='ru')\n",
    "moses_tokenized = mt.tokenize(test_text)\n",
    "tokenized['Moses'] = moses_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.5.0/ru_core_news_sm-3.5.0-py3-none-any.whl (15.3 MB)\n",
      "     ---------------------------------------- 0.0/15.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/15.3 MB 3.4 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 2.1/15.3 MB 5.9 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.9/15.3 MB 6.9 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 6.0/15.3 MB 7.7 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 8.7/15.3 MB 8.8 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 11.3/15.3 MB 9.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 13.9/15.3 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 15.3/15.3 MB 9.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ru-core-news-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: pymorphy3>=1.0.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ru-core-news-sm==3.5.0) (2.0.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (0.7.2)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.5.0) (2.4.417150.4580142)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.0.10)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.10.18)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (75.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\liza_n\\appdata\\roaming\\python\\python311\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.4.1)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.2.0)\n",
      "Requirement already satisfied: pathlib-abc==0.1.1 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pathy>=0.10.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\liza_n\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\liza_n\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.6.0,>=3.5.0->ru-core-news-sm==3.5.0) (1.2.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# spacy tokenizer\n",
    "!python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_sm')\n",
    "doc = nlp(test_text)\n",
    "spacy_tokenized = [token.text for token in doc]\n",
    "tokenized['Spacy'] =spacy_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# razdel tokenizer\n",
    "\n",
    "tokens = list(tokenize(test_text))\n",
    "razdel_tokenized = [_.text for _ in tokens]\n",
    "tokenized['Razdel'] = razdel_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer NLTK: ['Илья', 'Сегалович', 'родился', 'в', 'семье', 'советского', 'геофизика', 'В.И.Сегаловича', '.', 'В', 'физико-математической', 'школе', 'познакомился', 'с', 'будущим', 'сооснователем', '«', 'Яндекса', '»', 'Аркадием', 'Воложем', '.', 'В', '1981', 'г.', 'поступил', 'в', 'Московский', 'геологоразведочный', 'институт', 'им.Орджоникидзе', ',', 'где', 'когда-то', 'обучалось', '3', 'тыс.', 'студентов', '.']\n",
      "Tokenizer Moses: ['Илья', 'Сегалович', 'родился', 'в', 'семье', 'советского', 'геофизика', 'В.И.Сегаловича.', 'В', 'физико-математической', 'школе', 'познакомился', 'с', 'будущим', 'сооснователем', '«', 'Яндекса', '»', 'Аркадием', 'Воложем', '.', 'В', '1981', 'г.', 'поступил', 'в', 'Московский', 'геологоразведочный', 'институт', 'им.Орджоникидзе', ',', 'где', 'когда-то', 'обучалось', '3', 'тыс.', 'студентов', '.']\n",
      "Tokenizer Spacy: ['\\n', 'Илья', 'Сегалович', 'родился', 'в', 'семье', 'советского', 'геофизика', 'В.И.Сегаловича', '.', 'В', 'физико', '-', 'математической', 'школе', '\\n', 'познакомился', 'с', 'будущим', 'сооснователем', '«', 'Яндекса', '»', 'Аркадием', 'Воложем', '.', 'В', '1981', 'г.', 'поступил', 'в', 'Московский', '\\n', 'геологоразведочный', 'институт', 'им.', 'Орджоникидзе', ',', 'где', 'когда', '-', 'то', 'обучалось', '3', 'тыс.', 'студентов', '.', '\\n']\n",
      "Tokenizer Razdel: ['Илья', 'Сегалович', 'родился', 'в', 'семье', 'советского', 'геофизика', 'В', '.', 'И', '.', 'Сегаловича', '.', 'В', 'физико-математической', 'школе', 'познакомился', 'с', 'будущим', 'сооснователем', '«', 'Яндекса', '»', 'Аркадием', 'Воложем', '.', 'В', '1981', 'г', '.', 'поступил', 'в', 'Московский', 'геологоразведочный', 'институт', 'им', '.', 'Орджоникидзе', ',', 'где', 'когда-то', 'обучалось', '3', 'тыс', '.', 'студентов', '.']\n"
     ]
    }
   ],
   "source": [
    "#comparison\n",
    "for k, v in tokenized.items():\n",
    "    print(f'Tokenizer {k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not see too large difference between the results. All tokenizers handle well \"«»\" qutation marks, separating them from the word. Nevertheless, only Razdel separates period in the abbreviations, thus allowing to separately consider last names and abbreviated terms (in many cases they will coincide with the stemmed form of the word). We will use it for our preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in addition to the tokenizer, we will use a rule-based stemmer adapted for the Russian language (https://www.nltk.org/_modules/nltk/stem/snowball.html),\n",
    "# Russian stopword list from NLTK and a list of punctuation marks from string, adding quotation marks, \n",
    "# bringing all the tokens to lower case\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(text, remove_stop=True) -> list:\n",
    "    tokens = [_.text for _ in list(tokenize(text))]\n",
    "    \n",
    "    preprocessed =[]\n",
    "\n",
    "    stemmer = SnowballStemmer(\"russian\") \n",
    "    stop = stopwords.words(\"russian\")\n",
    "    punct = string.punctuation + \"«»\"\n",
    "    \n",
    "    for t in tokens:\n",
    "        if t in punct:\n",
    "            continue\n",
    "        if remove_stop and t.lower() in stop:\n",
    "            continue\n",
    "        preprocessed.append(stemmer.stem(t))\n",
    "\n",
    "    return preprocessed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['серг',\n",
       " 'аксен',\n",
       " 'сообщ',\n",
       " 'регистрац',\n",
       " 'крым',\n",
       " '247',\n",
       " 'нов',\n",
       " 'случа',\n",
       " 'COVID-19',\n",
       " 'глав',\n",
       " 'республик',\n",
       " 'крым',\n",
       " 'серг',\n",
       " 'аксен',\n",
       " 'сообщ',\n",
       " 'сво',\n",
       " 'официальн',\n",
       " 'страниц',\n",
       " 'социальн',\n",
       " 'сет',\n",
       " 'вконтакт',\n",
       " 'регистрац',\n",
       " '247',\n",
       " 'нов',\n",
       " 'случа',\n",
       " 'коронавирусн',\n",
       " 'инфекц',\n",
       " 'крым']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test on doc id ed4af92b-0039-453e-8f25-f359225da8e0\n",
    "\n",
    "text = 'Сергей Аксёнов сообщил о регистрации в Крыму 247 новых случаев COVID-19 Глава Республики Крым Сергей Аксёнов сообщил на своей официальной странице в социальной сети «ВКонтакте» о регистрации 247 новых случаев коронавирусной инфекции в Крыму.'\n",
    "\n",
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing was used for implementations of TF-IDF and BM25 and appeared appropriate. Nevertheless, the first tests of query expansion with relevance models revealed that among frequent tokens appended to queries to expand them appeared terms like \"—\", \"–\" (punctuation marks that turned out to be common in Russian texts, but not appearing in our punctuation list), \"котор\" and \"эт\" that correspond to stemmed versions of Russian words meaning \"which\" and \"this\". Such a query expansion lead to performance somewhat worse than on original queries. It showed that a modification of preprocessing function is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Liza_N\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's inspect NLTK Russian stopword list\n",
    "#from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "stopwords_nltk = stopwords.words(\"russian\")\n",
    "print(stopwords_nltk)\n",
    "print(len(stopwords_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'тоже', 'ночь', 'меньше', 'тринадцатый', 'всю', 'жизнь', 'опять', 'книга', 'нх', 'некоторый', 'наш', 'пойти', 'восемнадцать', 'спасибо', 'этой', 'собой', 'назад', 'ту', 'зачем', 'твой', 'за', 'любить', 'каждые', 'из', 'вами', 'низко', 'мог', 'вечер', 'десять', 'никуда', 'очень', 'давно', 'женщина', 'русский', 'ждать', 'ли', 'вода', 'сих', 'зато', 'эту', 'имеет', 'такие', 'страна', 'вдали', 'голос', 'деньги', 'а', 'были', 'друг', 'может', 'о', 'белый', 'важная', 'позже', 'год', 'еще', 'вон', 'большой', 'наше', 'они', 'сидеть', 'пол', 'часть', 'без', 'делать', 'во', 'м', 'девять', 'меля', 'две', 'комната', 'этими', 'меня', 'ним', 'тому', 'взгляд', 'отец', 'место', 'надо', 'перед', 'действительно', 'теми', 'так', 'вверх', 'решить', 'ребенок', 'говорить', 'сначала', 'слово', 'тебе', 'восемнадцатый', 'просто', 'нет', 'наконец', 'его', 'вернуться', 'от', 'сеаой', 'пока', 'раз', 'себе', 'однако', 'суть', 'шесть', 'около', 'свой', 'этого', 'уже', 'хороший', 'будь', 'семнадцатый', 'самими', 'близко', 'восемь', 'посреди', 'что', 'семь', 'после', 'иметь', 'которой', 'где', 'дом', 'кем', 'далеко', 'вся', 'кого', 'путь', 'есть', 'вдруг', 'её', 'жить', 'с кем', 'такой', 'одного', 'те', 'нему', 'подумать', 'тою', 'рука', 'тобою', 'занято', 'кто', 'хотя', 'идти', 'уж', 'больше', 'даром', 'третий', 'если', 'окно', 'тобой', 'был', 'дел', 'времени', 'это', 'я', 'тут', 'четвертый', 'вопрос', 'будете', 'куда', 'самой', 'нем', 'сами', 'лежать', 'такое', 'много', 'лицо', 'земля', 'быть', 'час', 'всем', 'этому', 'двадцать', 'старый', 'начать', 'наиболее', 'всё', 'первый', 'свои', 'маленький', 'многочисленный', 'девятнадцать', 'все еще', 'всего', 'всюду', 'никто', 'этим', 'обычно', 'тот', 'война', 'то', 'даже', 'три', 'само', 'тех', 'город', 'нужно', 'лучше', 'многочисленная', 'плечо', 'видеть', 'должный', 'знать', 'тем', 'но', 'собою', 'ничто', 'пять', 'мор', 'года', 'важный', 'не', 'впрочем', 'которых', 'оказаться', 'увидеть', 'стать', 'этих', 'другое', 'самих', 'вот', 'вас', 'ж', 'никакой', 'шестой', 'какая', 'смотреть', 'к', 'лишь', 'будто', 'говорил', 'долго', 'сама', 'нею', 'народ', 'при', 'алло', 'отсюда', 'тогда', 'до', 'наверху', 'вместе', 'можно', 'своей', 'могу', 'дело', 'чаще', 'ей', 'часто', 'второй', 'вообще', 'каждый', 'миллионов', 'сила', 'везде', 'делал', 'снова', 'иногда', 'свою', 'мои', 'слишком', 'против', 'многочисленное', 'многочисленные', 'бы', 'однажды', 'ещё', 'сторона', 'который', 'должен', 'четырнадцать', 'бывает', 'четырнадцатый', 'потом', 'через', 'уметь', 'со', 'которые', 'прекрасно', 'шестнадцать', 'найти', 'вокруг', 'недавно', 'довольно', 'восьмой', 'начала', 'другая', 'сказала', 'другой', 'время', 'конечно', 'в', 'затем', 'разве', 'дорога', 'вы', 'имя', 'же', 'случай', 'значит', 'машина', 'между', 'далекий', 'давать', 'самый', 'когда', 'здесь', 'сделать', 'нее', 'самого', 'самим', 'неё', 'всею', 'них', 'почему', 'рано', 'отовсюду', 'конец', 'девятнадцатый', 'день', 'будем', 'пожалуйста', 'твоё', 'пятый', 'твоя', 'совсем', 'кому', 'работать', 'занята', 'том', 'свое', 'почти', 'у', 'минута', 'сразу', 'нибудь', 'твои', 'правда', 'понимать', 'другие', 'несколько', 'ты', 'одиннадцать', 'писать', 'советский', 'него', 'раньше', 'значить', 'сегодня', 'моё', 'ней', 'думать', 'сказал', 'хоть', 'чуть', 'делаю', 'кругом', 'была', 'с', 'которого', 'себя', 'душа', 'пора', 'было', 'россия', 'могут', 'четыре', 'чему', 'об', 'новый', 'тебя', 'чем', 'двадцатый', 'вне', 'казаться', 'такая', 'москва', 'также', 'каждая', 'моя', 'подойди', 'одной', 'пятнадцатый', 'теперь', 'только', 'да', 'потому', 'особенно', 'там', 'имел', 'ком', 'утро', 'важное', 'говорит', 'саму', 'считать', 'человек', 'ничего', 'для', 'последний', 'ответить', 'оно', 'чтоб', 'ею', 'этот', 'над', 'мож', 'других', 'хорошо', 'являюсь', 'внизу', 'нельзя', 'чтобы', 'эта', 'будут', 'все', 'т', 'весь', 'ее', 'хотеть', 'наша', 'эти', 'голова', 'вид', 'немного', 'е', 'им', 'взять', 'друго', 'своего', 'нередко', 'должно', 'помнить', 'нога', 'ну', 'ваши', 'всему', 'рядом', 'самом', 'тысяч', 'нужный', 'буду', 'можхо', 'дальше', 'ведь', 'мы', 'работа', 'глаз', 'ни', 'важные', 'остаться', 'ваше', 'всегда', 'может быть', 'иди', 'c', 'товарищ', 'нас', 'и', 'непрерывно', 'про', 'самому', 'своих', 'недалеко', 'мимо', 'она', 'семнадцать', 'именно', 'дать', 'люди', 'откуда', 'получить', 'хотел бы', 'кроме', 'седьмой', 'какой', 'свет', 'будешь', 'ряд', 'он', 'всеми', 'нам', 'улица', 'десятый', 'мне', 'шестнадцатый', 'понять', 'посмотреть', 'каждое', 'лет', 'процентов', 'мной', 'мир', 'ему', 'стоять', 'жена', 'кажется', 'мать', 'сколько', 'стал', 'их', 'девятый', 'стол', 'менее', 'вниз', 'всех', 'г', 'мною', 'по', 'двенадцать', 'мочь', 'более', 'ваш', 'дверь', 'туда', 'мало', 'ниже', 'бывь', 'которая', 'пор', 'та', 'таки', 'этом', 'будет', 'видел', 'два', 'на', 'один', 'того', 'двух', 'ваша', 'или', 'никогда', 'ними', 'вам', 'сейчас', 'под', 'двенадцатый', 'чего', 'сам', 'нами', 'главный', 'выйти', 'оба', 'как', 'спросить', 'сказать', 'ими', 'пятнадцать', 'мира', 'заняты', 'мой', 'занят', 'одиннадцатый', 'году', 'наши', 'тринадцать', 'хочешь'}\n",
      "559\n"
     ]
    }
   ],
   "source": [
    "# an alternative stopword list for Russian language was found at https://github.com/stopwords-iso/stopwords-iso/blob/master/README.md\n",
    "stopwords_ru = stopwordsiso.stopwords(\"ru\")\n",
    "print(stopwords_ru)\n",
    "print(len(stopwords_ru))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative stopword list is considerably longer and more comprehensive than the NLTK one. We will upgrade our preprocessing function by replacing the stopword list and including discovered punctuation into the punctuation mark list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(text, remove_stop: bool=True) -> list:\n",
    "    \"\"\"\n",
    "    :text: str, text of the corresponding document\n",
    "    :param remove_stop: bool indicating if stopwords should be removed (default True)\n",
    "\n",
    "    :return: list(str) of tokens, stemmed, with removed punctuation\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    tokens = [_.text for _ in list(tokenize(text))]\n",
    "\n",
    "    preprocessed =[]\n",
    "\n",
    "    stemmer = SnowballStemmer(\"russian\")\n",
    "    stopwords_ru = stopwords(\"ru\")\n",
    "    punct = string.punctuation + \"«»\" + \"—\" + '–'\n",
    "\n",
    "    for t in tokens:\n",
    "        if t in punct:\n",
    "            continue\n",
    "        if remove_stop and t.lower() in stopwords_ru:\n",
    "            continue\n",
    "        preprocessed.append(stemmer.stem(t))\n",
    "\n",
    "    return preprocessed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All experiments in CLIR word-overlap based methods were rerun with the updated preprocessing function. For TF-IDF and BM25 there was insignificant increase in performance metrics. At the same time, the quality of expanded queries improved, leading to relevance model based techniques outperform the original BM25 implementation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
