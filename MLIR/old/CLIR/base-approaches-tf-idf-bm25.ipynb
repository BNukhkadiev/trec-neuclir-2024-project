{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import torch\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import string\n",
    "from rank_bm25 import BM25Okapi\n",
    "import ir_measures\n",
    "from ir_measures import nDCG, MAP, RBP, Recall, Qrel, ScoredDoc\n",
    "from itertools import chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id='neuclir/1/ru/trec-2023', provides=['docs', 'queries', 'qrels'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ir_datasets.load(\"neuclir/1/ru/trec-2023\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d32a59a55742ed92ccc699df95940e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "russian_documents = [(doc.doc_id, doc.title, doc.text) for doc in tqdm(dataset.docs_iter())]\n",
    "english_queries = [(query.query_id, query.title) for query in dataset.queries_iter()]\n",
    "qrels = [(qrel.query_id, qrel.doc_id, qrel.relevance) for qrel in dataset.qrels_iter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9fc2a4a7bc48078d87e1e899683652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25634 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c315ef9376f400a853c17d0df30312b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "24871"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels_ids = {entry[1] for entry in tqdm(qrels)}\n",
    "russian_documents_subset = [doc for doc in tqdm(russian_documents) if doc[0] in qrels_ids]\n",
    "len(russian_documents_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(qrels, result):\n",
    "    qrels = [\n",
    "        Qrel(query_id=query_id, doc_id=doc_id, relevance=relevance)\n",
    "        for query_id, doc_id, relevance, iterations in qrels   \n",
    "    ]\n",
    "\n",
    "    runs = [\n",
    "        ScoredDoc(query_id=query_id, doc_id=doc_id, score=score)\n",
    "        for query_id, doc_id, score in result\n",
    "    ]\n",
    "#     scores = ir_measures.calc_aggregate([nDCG@20, MAP, RBP(rel=1), Recall@100, Recall@1000], qrels, runs)\n",
    "    scores = ir_measures.calc_aggregate([nDCG@20, MAP, Recall@100, Recall@1000], qrels, runs)\n",
    "\n",
    "\n",
    "    return scores\n",
    "    \n",
    "    \n",
    "\n",
    "def print_document(document_id):\n",
    "    print(next((doc for doc in russian_documents if doc[0] == document_id), None))\n",
    "\n",
    "\n",
    "def translate_query(query):\n",
    "    translated_text = GoogleTranslator(source='auto', target='ru').translate(query[1]) \n",
    "    translated_tuple = (query[0], translated_text)\n",
    "\n",
    "    return translated_tuple\n",
    "\n",
    "def tokenize(text):\n",
    "\n",
    "    query_tokens = text[1].split()\n",
    "\n",
    "    # Define Russian stopwords\n",
    "    russian_stopwords = set(stopwords.words('russian'))\n",
    "\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [token.translate(translator).lower() for token in query_tokens if token.lower() not in russian_stopwords]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def convert_to_score(result, qrels):\n",
    "    combined_dict = { (qid, docid): relevance for qid, docid, relevance in result}\n",
    "\n",
    "    correct_matches = 0\n",
    "    total = 0  \n",
    "\n",
    "    # Compare qrels with combined_documents\n",
    "    for qid, docid, true_relevance in qrels:\n",
    "        if (qid, docid) in combined_dict:\n",
    "            predicted_relevance = combined_dict[(qid, docid)]\n",
    "            if predicted_relevance == true_relevance:\n",
    "                correct_matches += 1\n",
    "            total += 1  \n",
    "        \n",
    "    accuracy = correct_matches / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def combine_documents(documents):\n",
    "    combine_documents =  list(map(lambda doc: (doc[0], doc[1] + doc[2]), documents))\n",
    "    return combine_documents\n",
    "\n",
    "def assign_rank_tf_idf(value):\n",
    "    if value >= 0.1:\n",
    "        return 3\n",
    "    elif value >= 0.5:\n",
    "        return 2\n",
    "    elif value >= 0.01:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def assign_rank(value):\n",
    "    if value >= 0.2:\n",
    "        return 3\n",
    "    elif value >= 0.11:\n",
    "        return 2\n",
    "    elif value >= 0.05:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def assign_rank_bm25(value):\n",
    "    if value >= 10:\n",
    "        return 3\n",
    "    elif value >= 6:\n",
    "        return 2\n",
    "    elif value >= 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(query, documents):\n",
    "    # Translate the query, assuming this is a function you have defined elsewhere\n",
    "    translated_query = translate_query(query)\n",
    "\n",
    "    tokenized_query = tokenize(translated_query)\n",
    "\n",
    "    # Initialize a dictionary to store documents for each word\n",
    "    word_to_documents = {}\n",
    "\n",
    "    # Iterate over each word in the query\n",
    "    for word in tokenized_query:\n",
    "        # Check each document for the word\n",
    "        for doc_id, combined_text in documents:\n",
    "            # If the word is found in the combined text (case-sensitive check)\n",
    "            if word in combined_text:\n",
    "                if word not in word_to_documents:\n",
    "                    word_to_documents[word] = []\n",
    "                word_to_documents[word].append(doc_id)\n",
    "\n",
    "    # Create a dictionary to store the document frequencies (relevance score)\n",
    "    document_frequency = {doc_id: 0 for doc_id, _ in documents}\n",
    "\n",
    "    # For each word found in the documents, increment the score of the relevant documents\n",
    "    for word, docs in word_to_documents.items():\n",
    "        for doc_id in docs:\n",
    "            document_frequency[doc_id] += 1  # Increment relevance score for documents that contain the word\n",
    "\n",
    "    # Sort documents based on relevance (document frequency)\n",
    "    sorted_documents = sorted(document_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Format the final output, giving a max score of 3 if the relevance is above 3, else leave as is\n",
    "    formatted_docs = [(query[0], doc_id, min(relevance, 3)) for doc_id, relevance in sorted_documents]\n",
    "\n",
    "    return formatted_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(query, documents):\n",
    "    # Extract the query and documents texts\n",
    "    query_id, query_text = query\n",
    "    doc_ids, doc_texts = zip(*documents)  # unzip the document tuples into ids and texts\n",
    "\n",
    "    # Combine the query text with the document texts for vectorization\n",
    "    texts = [query_text] + list(doc_texts)\n",
    "\n",
    "    # Create the TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the text data\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # Extract the query vector (the first row in the matrix)\n",
    "    query_vector = tfidf_matrix[0:1]\n",
    "\n",
    "    # Compute the cosine similarity between the query vector and the document vectors\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix[1:]).flatten()\n",
    "\n",
    "    # Pair each document id with its corresponding cosine similarity score\n",
    "    scored_documents = [(doc_id, similarity) for doc_id, similarity in zip(doc_ids, cosine_similarities)]\n",
    "\n",
    "    # Sort documents by similarity score in descending order\n",
    "    sorted_documents = sorted(scored_documents, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "\n",
    "    ranked_data = [(query_id, uuid, assign_rank(value)) for uuid, value in sorted_documents]\n",
    "\n",
    "\n",
    "    return ranked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(query, documents):\n",
    "    # query = english_queries[2]\n",
    "\n",
    "    # Translate the query (assumed to return a tuple with the query number and tokens)\n",
    "    translated_query = translate_query(query)\n",
    "\n",
    "    query_number = translated_query[0]  \n",
    "    query_tokens = tokenize(translated_query) \n",
    "    document_ids = [item[0] for item in documents] \n",
    "\n",
    "    # Tokenize documents and initialize BM25\n",
    "    corpus = [tokenize(doc) for doc in documents]\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "\n",
    "    # Get BM25 scores for the query tokens\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "    # Combine query_number, document_ids, and scores into the required format\n",
    "    scored_documents = [(doc_id, score) for doc_id, score in zip(document_ids, scores)]\n",
    "\n",
    "    sorted_documents = sorted(scored_documents, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    ranked_data = [(query_number, uuid, assign_rank_bm25(value)) for uuid, value in sorted_documents]\n",
    "\n",
    "\n",
    "    return ranked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm11(query, documents):\n",
    "\n",
    "    translated_query = translate_query(query)\n",
    "\n",
    "    k=1.2\n",
    "    # Tokenize the query and documents\n",
    "    query_terms = translated_query[1].lower().split()  # Convert to lowercase and split\n",
    "    query_terms_count = Counter(query_terms)  # Count occurrences of query terms\n",
    "    \n",
    "    # Calculate average document length\n",
    "    doc_lengths = [len(doc[1].split()) for doc in documents]\n",
    "    avg_doc_length = sum(doc_lengths) / len(documents)\n",
    "    \n",
    "    # Function to compute the term frequency in a document\n",
    "    def term_frequency(term, doc):\n",
    "        return doc.split().count(term)\n",
    "\n",
    "    # Initialize a list to store BM11 scores for each document\n",
    "    scores = []\n",
    "    \n",
    "    # Calculate BM11 score for each document\n",
    "    for doc_id, doc_text in tqdm(documents):\n",
    "        doc_length = len(doc_text.split())\n",
    "        score = 0\n",
    "        for term in query_terms_count:\n",
    "            # Term frequency in the document\n",
    "            f_td = term_frequency(term, doc_text)\n",
    "            if f_td > 0:  # Only consider terms that appear in the document\n",
    "                weight_t = query_terms_count[term]  # The weight of the term in the query\n",
    "                # BM11 formula\n",
    "                score += (f_td * (k + 1)) / (f_td + k * (doc_length / avg_doc_length)) * weight_t\n",
    "        scores.append((doc_id, score))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_documents = combine_documents(russian_documents_subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbba179883d64d678151f3305ebc3d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_ranked_documents_inverted_index = []\n",
    "for query in tqdm(english_queries):\n",
    "    scores_inverted_index = inverted_index(query, combined_documents)\n",
    "    all_ranked_documents_inverted_index.append(scores_inverted_index)\n",
    "\n",
    "flat_list_inverted_index = list(chain.from_iterable(all_ranked_documents_inverted_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.5265270604239012,\n",
       " AP: 0.07992448263972612,\n",
       " R@100: 0.19601926707180986,\n",
       " nDCG@20: 0.12663033443370092}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, flat_list_inverted_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39bd705c8cb4d188d33f7510568efaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_ranked_documents_tfidf = []\n",
    "for query in tqdm(english_queries):\n",
    "    scores_tfidf = tf_idf(query, combined_documents)\n",
    "    all_ranked_documents_tfidf.append(scores_tfidf)\n",
    "\n",
    "flat_list_tfidf = list(chain.from_iterable(all_ranked_documents_tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.17150749976575816,\n",
       " AP: 0.05580265335255272,\n",
       " R@100: 0.1106283437736037,\n",
       " nDCG@20: 0.11141536093960405}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, flat_list_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3ba3e2171f4758b401ca3970066669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_ranked_documents_bm25 = []\n",
    "for query in tqdm(english_queries):\n",
    "    scores_bm25 = bm25(query, combined_documents)\n",
    "    all_ranked_documents_bm25.append(scores_bm25)\n",
    "\n",
    "flat_list_bm25 = list(chain.from_iterable(all_ranked_documents_bm25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.7050313456632903,\n",
       " AP: 0.1401383178583815,\n",
       " R@100: 0.3497669872921656,\n",
       " nDCG@20: 0.1814989022326702}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, flat_list_bm25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
