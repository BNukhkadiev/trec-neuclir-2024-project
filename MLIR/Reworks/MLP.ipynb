{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ir_datasets\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from ir_measures import nDCG, MAP, RBP, Recall, Qrel, ScoredDoc, calc_aggregate\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_documents = pd.read_csv('data/sbert_documents.csv')\n",
    "df_queries = pd.read_csv('data/queries.csv')\n",
    "dataset = ir_datasets.load(\"neuclir/1/multi/trec-2023\")\n",
    "english_queries = [(query.query_id, query.title) for query in dataset.queries_iter()]\n",
    "\n",
    "qrels = [(qrel.query_id, qrel.doc_id, qrel.relevance) for qrel in dataset.qrels_iter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ir_datasets.load(\"neuclir/1/multi/trec-2023\")\n",
    "\n",
    "english_queries = [(query.query_id, query.title) for query in dataset.queries_iter()]\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "query_embeddings = model.encode([query[1] for query in english_queries], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries['query_embedding'] = df_queries['query_embedding'].apply(lambda x: torch.tensor(ast.literal_eval(x.split('tensor')[1])))\n",
    "df_documents['content_embedding'] = df_documents['content_embedding'].apply(lambda x: torch.tensor(ast.literal_eval(x.split('tensor')[1])))\n",
    "df_documents['title_embedding'] = df_documents['title_embedding'].apply(lambda x: torch.tensor(ast.literal_eval(x.split('tensor')[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(qrels, result):\n",
    "    qrels = [\n",
    "        Qrel(query_id=query_id, doc_id=doc_id, relevance=relevance)\n",
    "        for query_id, doc_id, relevance in qrels   \n",
    "    ]\n",
    "\n",
    "    runs = [\n",
    "        ScoredDoc(query_id=query_id, doc_id=doc_id, score=score)\n",
    "        for query_id, doc_id, score in result\n",
    "    ]\n",
    "    scores = calc_aggregate([nDCG@20, MAP, RBP(rel=1), Recall@100, Recall@1000], qrels, runs)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82b33f150e141af96383accb121fe9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_feature_set = []\n",
    "train_labels = []\n",
    "\n",
    "# Iterate through query 200-220 as training data\n",
    "for query_id, doc_id, score in tqdm(qrels):\n",
    "    if 200 <= int(query_id) <= 220:\n",
    "        # Retrieve the corresponding query embedding and document embedding\n",
    "        query_embedding = df_queries.loc[df_queries['id'] == int(query_id), 'query_embedding'].values[0]\n",
    "        content_embedding = df_documents.loc[df_documents['id'] == doc_id, 'content_embedding'].values[0]\n",
    "\n",
    "        # Concatenate the query embedding and document content embedding\n",
    "        feature_vector = np.concatenate((query_embedding, content_embedding))\n",
    "        \n",
    "        # Append the feature vector and score\n",
    "        train_feature_set.append(feature_vector)\n",
    "        train_labels.append(score)\n",
    "\n",
    "train_feature_set = np.array(train_feature_set)  # Numpy array of feature vectors\n",
    "train_labels = np.array(train_labels)  # Numpy array of scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a519eeded994ac8ae960c25de16d909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_feature_set = []\n",
    "test_labels = []\n",
    "test_doc_ids = []\n",
    "test_query_id = []\n",
    "for query_id, doc_id, score in tqdm(qrels):\n",
    "    # Iterate through query 221-275 as test data\n",
    "    if 221 <= int(query_id) <= 275:\n",
    "        # Retrieve the corresponding query embedding and document embedding\n",
    "        query_embedding = df_queries.loc[df_queries['id'] == int(query_id), 'query_embedding'].values[0]\n",
    "        content_embedding = df_documents.loc[df_documents['id'] == doc_id, 'content_embedding'].values[0]\n",
    "        test_doc_ids.append(doc_id)\n",
    "        test_query_id.append(query_id)\n",
    "        \n",
    "        # Concatenate query embedding and document content embedding\n",
    "        feature_vector = np.concatenate((query_embedding, content_embedding))\n",
    "        \n",
    "        # Append the feature vector and score\n",
    "        test_feature_set.append(feature_vector)\n",
    "        test_labels.append(score)\n",
    "\n",
    "# Convert feature_set and labels into appropriate formats\n",
    "test_feature_set = np.array(test_feature_set)  \n",
    "test_labels = np.array(test_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentQueryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, feature_embeddings, relevance_scores):\n",
    "        self.feature_embeddings = feature_embeddings\n",
    "        self.relevance_scores = relevance_scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.relevance_scores)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_embeddings = torch.tensor(self.feature_embeddings[idx], dtype=torch.float32)\n",
    "        relevance_score = torch.tensor(self.relevance_scores[idx], dtype=torch.float32)\n",
    "        return feature_embeddings, relevance_score\n",
    "    \n",
    "dataset = DocumentQueryDataset(train_feature_set, train_labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        self.input_size = 768\n",
    "        \n",
    "        self.hidden1 = torch.nn.Linear(self.input_size, self.input_size)\n",
    "        self.hidden2 = torch.nn.Linear(self.input_size, int(self.input_size/2))\n",
    "        self.output = torch.nn.Linear(int(self.input_size/2), 4)  \n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPModel()  \n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6278221011161804\n",
      "Epoch 2/5, Loss: 0.906505286693573\n",
      "Epoch 3/5, Loss: 0.5591813921928406\n",
      "Epoch 4/5, Loss: 0.487682968378067\n",
      "Epoch 5/5, Loss: 0.8473197221755981\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for data, target in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        target = target.long()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  \n",
    "\n",
    "features = torch.tensor(test_feature_set, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():  \n",
    "    output = model(features) \n",
    "\n",
    "    predicted_class = torch.argmax(output, dim=1).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test qrels into correct format\n",
    "test_qrels = [item for item in qrels if int(item[0]) >= 221]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into correct format\n",
    "mlp_scores = list(zip(test_query_id, test_doc_ids, predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nDCG@20: 0.022632390815729223,\n",
       " R@100: 0.06808907513492216,\n",
       " R@1000: 0.6295431973386293,\n",
       " RBP(rel=1): 0.022045023113816155,\n",
       " AP: 0.016838619148803944}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(mlp_scores, test_qrels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
