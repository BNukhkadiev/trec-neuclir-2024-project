{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ir_datasets\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from ir_measures import nDCG, MAP, RBP, Recall, Qrel, ScoredDoc, calc_aggregate\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "from scipy.spatial.distance import euclidean\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = ir_datasets.load(\"neuclir/1/multi/trec-2023\")\n",
    "english_queries = [\n",
    "    (\n",
    "        query.query_id, \n",
    "        query.title, \n",
    "        query.description, \n",
    "        query.fa_mt_title, \n",
    "        query.fa_mt_description, \n",
    "        query.ru_mt_title, \n",
    "        query.ru_mt_description, \n",
    "        query.zh_mt_title, \n",
    "        query.zh_mt_description, \n",
    "    ) \n",
    "    for query in dataset.queries_iter()\n",
    "]\n",
    "qrels = [(qrel.query_id, qrel.doc_id, qrel.relevance, qrel.iteration) for qrel in dataset.qrels_iter()]\n",
    "\n",
    "# Load locally saved embeddings from sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 \n",
    "with open('data/document_embeddings.pkl', 'rb') as file:\n",
    "    document_embeddings = pickle.load(file)\n",
    "\n",
    "with open('data/title_embeddings.pkl', 'rb') as file:\n",
    "    title_embeddings = pickle.load(file)\n",
    "\n",
    "with open('data/multi-subset.pkl', 'rb') as file:\n",
    "    multi_subset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b96f62175f94ed0ae15cd4a61d2dbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76913 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# Create query embeddinsg using sentence transformer (same procedure was applied to obtain the priorly loaded embeddings)\n",
    "query_embeddings = model.encode([query[1] for query in english_queries], convert_to_tensor=True)\n",
    "\n",
    "# Construct qrel dataframe\n",
    "qrels_ids = []\n",
    "document_ids = []\n",
    "scores = []\n",
    "iterations = []\n",
    "\n",
    "\n",
    "for qrel in qrels:\n",
    "    qrels_ids.append(qrel[0])\n",
    "    document_ids.append(qrel[1])\n",
    "    scores.append(qrel[2])\n",
    "    iterations.append(qrel[3])\n",
    "\n",
    "df_qrels = pd.DataFrame({\n",
    "    'id':  qrels_ids, \n",
    "    'document_id': document_ids, \n",
    "    'score': scores,\n",
    "    'iteration': iterations\n",
    "})\n",
    "\n",
    "# construct documents dataframe\n",
    "multi_subset_ids = []\n",
    "multi_subset_title = []\n",
    "multi_subset_content = []\n",
    "\n",
    "for x in tqdm(multi_subset):\n",
    "    multi_subset_ids.append(x[0])\n",
    "    multi_subset_title.append(x[1])\n",
    "    multi_subset_content.append(x[2])    \n",
    "\n",
    "df_documents = pd.DataFrame({\n",
    "    'id': multi_subset_ids,\n",
    "    'title': multi_subset_title,\n",
    "    'content': multi_subset_content,\n",
    "    'title_embedding': title_embeddings,\n",
    "    'content_embedding': document_embeddings\n",
    "})\n",
    "\n",
    "# Construct query dataset\n",
    "queries_ids = []\n",
    "queries_query = []\n",
    "queries_description = []\n",
    "queries_fa_mt_title = []\n",
    "queries_fa_mt_description = []\n",
    "queries_ru_mt_title = []\n",
    "queries_ru_mt_description = []\n",
    "queries_zh_mt_title = []\n",
    "queries_zh_mt_description = []\n",
    "\n",
    "for x in english_queries:\n",
    "    queries_ids.append(x[0])\n",
    "    queries_query.append(x[1])\n",
    "    queries_description.append(x[2])\n",
    "    queries_fa_mt_title.append(x[3])\n",
    "    queries_fa_mt_description.append(x[4])\n",
    "    queries_ru_mt_title.append(x[5])\n",
    "    queries_ru_mt_description.append(x[6])\n",
    "    queries_zh_mt_title.append(x[7])\n",
    "    queries_zh_mt_description.append(x[8])\n",
    "    \n",
    "\n",
    "\n",
    "df_queries = pd.DataFrame(\n",
    "    english_queries, \n",
    "    columns=[\n",
    "        'query_id', \n",
    "        'title', \n",
    "        'description', \n",
    "        'fa_mt_title', \n",
    "        'fa_mt_description', \n",
    "        'ru_mt_title', \n",
    "        'ru_mt_description', \n",
    "        'zh_mt_title', \n",
    "        'zh_mt_description', \n",
    "    ]\n",
    ")\n",
    "\n",
    "df_queries = pd.DataFrame({\n",
    "    'id':  queries_ids, \n",
    "    'query': queries_query,\n",
    "    'description': queries_description,\n",
    "    'fa_mt_title': queries_fa_mt_title,\n",
    "    'fa_mt_description': queries_fa_mt_description,\n",
    "    'ru_mt_title': queries_ru_mt_title,\n",
    "    'ru_mt_description': queries_ru_mt_description,\n",
    "    'zh_mt_title': queries_zh_mt_title,\n",
    "    'zh_mt_description': queries_zh_mt_description,\n",
    "    'query_embedding': list(query_embeddings)\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join df_qrels and df_documents together via document id\n",
    "df_merged = pd.merge(df_qrels, df_documents, how='left', left_on='document_id', right_on='id')\n",
    "\n",
    "df_merged.rename(columns={'title': 'document_title'}, inplace=True)\n",
    "\n",
    "# Merge joined dataframe together via query id\n",
    "df_merged = pd.merge(df_merged, df_queries, how='left', left_on='id_x', right_on='id')\n",
    "\n",
    "# Assign query translation based on document language\n",
    "def get_mt_title(row):\n",
    "    if row['iteration'] == 'fas':\n",
    "        return row['fa_mt_title']\n",
    "    elif row['iteration'] == 'rus':\n",
    "        return row['ru_mt_title']\n",
    "    elif row['iteration'] == 'zho':\n",
    "        return row['zh_mt_title']\n",
    "    else:\n",
    "        return None \n",
    " \n",
    "df_merged['query_title_translated'] = df_merged.apply(get_mt_title, axis=1)\n",
    "\n",
    "# Drop columns that are not usefull anymore\n",
    "df_merged.drop(columns=['fa_mt_title', 'ru_mt_title', 'zh_mt_title', 'fa_mt_description', 'ru_mt_description', 'zh_mt_description', 'id_y', 'id_x'], inplace=True)\n",
    "\n",
    "# Merge df_qrels with df_documents on document id\n",
    "final_merged_df = pd.merge(df_merged, df_qrels, on=['id', 'document_id'])\n",
    "final_merged_df.rename(columns={'score_x': 'score', 'iteration_x': 'iteration'})\n",
    "\n",
    "final_merged_df.drop(columns=['score_y', 'iteration_y'], inplace=True)\n",
    "\n",
    "# delete iteration column because it is not used anymore\n",
    "qrels = [t[:-1] for t in qrels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate(qrels, result):\n",
    "    qrels = [\n",
    "        Qrel(query_id=query_id, doc_id=doc_id, relevance=relevance)\n",
    "        for query_id, doc_id, relevance in qrels   \n",
    "    ]\n",
    "\n",
    "    runs = [\n",
    "        ScoredDoc(query_id=query_id, doc_id=doc_id, score=score)\n",
    "        for query_id, doc_id, score in result\n",
    "    ]\n",
    "    scores = calc_aggregate([nDCG@20, MAP, RBP(rel=1), Recall@100, Recall@1000], qrels, runs)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensors to numpy arrays for computational sparsity\n",
    "final_merged_df['title_embedding'] = final_merged_df['title_embedding'].apply(lambda x: x.numpy() if hasattr(x, 'numpy') else np.array(x))\n",
    "final_merged_df['content_embedding'] = final_merged_df['content_embedding'].apply(lambda x: x.numpy() if hasattr(x, 'numpy') else np.array(x))\n",
    "final_merged_df['query_embedding'] = final_merged_df['query_embedding'].apply(lambda x: x.numpy() if hasattr(x, 'numpy') else np.array(x))\n",
    "\n",
    "# aggregate title embedding and content embedding  \n",
    "final_merged_df['mean_title_content_embedding'] = final_merged_df.apply(\n",
    "    lambda row: np.mean([row['title_embedding'], row['content_embedding']], axis=0),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cosine similarity function as similarity function\n",
    "def calculate_cosine_similarity(row, embedding_type):\n",
    "    query_vec = np.array(row['query_embedding']).reshape(1, -1)  # Convert list to numpy array and reshape for cosine_similarity\n",
    "    title_vec = np.array(row[embedding_type]).reshape(1, -1)  \n",
    "    return cosine_similarity(query_vec, title_vec)[0][0]\n",
    "\n",
    "# Define euclidean distance as similarity function\n",
    "def calculate_euclidean_distance(row, embedding_type):\n",
    "    query_vec = np.array(row['query_embedding'])  \n",
    "    title_vec = np.array(row[embedding_type])  \n",
    "    return euclidean(query_vec, title_vec)\n",
    "\n",
    "# Define dot product as similarity function\n",
    "def calculate_dot_product(row, embedding_type):\n",
    "    query_vec = np.array(row['query_embedding']) \n",
    "    title_vec = np.array(row[embedding_type])  \n",
    "    return np.dot(query_vec, title_vec)\n",
    "\n",
    "\n",
    "# Apply individual similarity measures and save in dataframe for easy access\n",
    "final_merged_df['title_cosine_sim'] = final_merged_df.apply(calculate_cosine_similarity,  axis=1, embedding_type='title_embedding')\n",
    "final_merged_df['content_cosine_sim'] = final_merged_df.apply(calculate_cosine_similarity,  axis=1, embedding_type='content_embedding')\n",
    "final_merged_df['mean_title_content_embedding_cosine_sim'] = final_merged_df.apply(calculate_cosine_similarity,  axis=1, embedding_type='mean_title_content_embedding')\n",
    "\n",
    "final_merged_df['title_euclidean_sim'] = final_merged_df.apply(calculate_euclidean_distance,  axis=1, embedding_type='title_embedding')\n",
    "final_merged_df['content_euclidean_sim'] = final_merged_df.apply(calculate_euclidean_distance,  axis=1, embedding_type='content_embedding')\n",
    "final_merged_df['mean_title_content_embedding_euclidean_sim'] = final_merged_df.apply(calculate_euclidean_distance,  axis=1, embedding_type='mean_title_content_embedding')\n",
    "\n",
    "final_merged_df['title_dot_product'] = final_merged_df.apply(calculate_dot_product,  axis=1, embedding_type='title_embedding')\n",
    "final_merged_df['content_dot_product'] = final_merged_df.apply(calculate_dot_product,  axis=1, embedding_type='content_embedding')\n",
    "final_merged_df['mean_title_content_embedding_dot_product'] = final_merged_df.apply(calculate_dot_product,  axis=1, embedding_type='mean_title_content_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tuple of each measure and sort descendingly by score\n",
    "sorted_df_cosine_sim_title = final_merged_df.sort_values(by=['id', 'title_cosine_sim'], ascending=[True, False])\n",
    "sbert_title_cosine_runs = list(sorted_df_cosine_sim_title[['id', 'document_id', 'title_cosine_sim']].itertuples(index=False, name=None))\n",
    "\n",
    "sorted_df_cosine_sim_content = final_merged_df.sort_values(by=['id', 'content_cosine_sim'], ascending=[True, False])\n",
    "sbert_content_cosine_runs = list(sorted_df_cosine_sim_content[['id', 'document_id', 'content_cosine_sim']].itertuples(index=False, name=None))\n",
    "\n",
    "sorted_df_cosine_sim_title_and_content = final_merged_df.sort_values(by=['id', 'mean_title_content_embedding_cosine_sim'], ascending=[True, False])\n",
    "sbert_title_and_content_cosine_runs = list(sorted_df_cosine_sim_title_and_content[['id', 'document_id', 'mean_title_content_embedding_cosine_sim']].itertuples(index=False, name=None))\n",
    "\n",
    "sorted_df_euclidean_sim_title = final_merged_df.sort_values(by=['id', 'title_euclidean_sim'], ascending=[True, False])\n",
    "sbert_title_euclidean_runs = list(sorted_df_euclidean_sim_title[['id', 'document_id', 'title_euclidean_sim']].itertuples(index=False, name=None))\n",
    "\n",
    "sorted_df_euclidean_sim_content = final_merged_df.sort_values(by=['id', 'content_euclidean_sim'], ascending=[True, False])\n",
    "sbert_content_euclidean_runs = list(sorted_df_euclidean_sim_content[['id', 'document_id', 'content_euclidean_sim']].itertuples(index=False, name=None))\n",
    "\n",
    "sorted_df_euclidean_sim_title_and_content = final_merged_df.sort_values(by=['id', 'mean_title_content_embedding_euclidean_sim'], ascending=[True, False])\n",
    "sbert_title_and_content_euclidean_runs = list(sorted_df_euclidean_sim_title_and_content[['id', 'document_id', 'mean_title_content_embedding_euclidean_sim']].itertuples(index=False, name=None))\n",
    "\n",
    "sorted_df_dot_product_title = final_merged_df.sort_values(by=['id', 'title_dot_product'], ascending=[True, True])\n",
    "sbert_title_dot_product_runs = list(sorted_df_dot_product_title[['id', 'document_id', 'title_dot_product']].itertuples(index=False, name=None))\n",
    "\n",
    "sorted_df_dot_product_content = final_merged_df.sort_values(by=['id', 'content_dot_product'], ascending=[True, True])\n",
    "sbert_content_dot_product_runs = list(sorted_df_dot_product_content[['id', 'document_id', 'content_dot_product']].itertuples(index=False, name=None))\n",
    "\n",
    "sorted_df_dot_product_title_and_content = final_merged_df.sort_values(by=['id', 'mean_title_content_embedding_dot_product'], ascending=[True, True])\n",
    "sbert_title_and_content_dot_product_runs = list(sorted_df_euclidean_sim_title_and_content[['id', 'document_id', 'mean_title_content_embedding_dot_product']].itertuples(index=False, name=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.9289491760379356,\n",
       " AP: 0.26605608571641354,\n",
       " nDCG@20: 0.28265923222559025,\n",
       " RBP(rel=1): 0.41031624199604577,\n",
       " R@100: 0.19239881908238096}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, sbert_title_cosine_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.9370592256561111,\n",
       " AP: 0.3071181697275949,\n",
       " nDCG@20: 0.3265541608066136,\n",
       " RBP(rel=1): 0.4741240482870999,\n",
       " R@100: 0.22473586747183774}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, sbert_content_cosine_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.9399395199920656,\n",
       " AP: 0.29486777446657086,\n",
       " nDCG@20: 0.31654892237035054,\n",
       " RBP(rel=1): 0.46047940557638006,\n",
       " R@100: 0.21728120370757278}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, sbert_title_and_content_cosine_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.8282944435480063,\n",
       " AP: 0.15946156275371742,\n",
       " nDCG@20: 0.06722090656628024,\n",
       " RBP(rel=1): 0.11884350097582393,\n",
       " R@100: 0.07406646992555975}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, sbert_title_euclidean_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.8132227545869642,\n",
       " AP: 0.14759415779477406,\n",
       " nDCG@20: 0.048247301288433825,\n",
       " RBP(rel=1): 0.08714808947395569,\n",
       " R@100: 0.05622804206503338}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, sbert_content_euclidean_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.8138366184703791,\n",
       " AP: 0.1501234338457982,\n",
       " nDCG@20: 0.05126840667138022,\n",
       " RBP(rel=1): 0.09045887275342691,\n",
       " R@100: 0.054912916481281895}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, sbert_title_and_content_euclidean_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.928911568640813,\n",
       " AP: 0.25618383141609175,\n",
       " nDCG@20: 0.26355405266151966,\n",
       " RBP(rel=1): 0.37382781401889587,\n",
       " R@100: 0.18322231526506277}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, sbert_title_dot_product_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.939474688869609,\n",
       " AP: 0.30063341567550045,\n",
       " nDCG@20: 0.3067217994326334,\n",
       " RBP(rel=1): 0.43431829309903663,\n",
       " R@100: 0.220252211939205}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, sbert_content_dot_product_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.940527634142903,\n",
       " AP: 0.28308758924802957,\n",
       " nDCG@20: 0.29465933186119225,\n",
       " RBP(rel=1): 0.4146459939783991,\n",
       " R@100: 0.20627482589211824}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(qrels, sbert_title_and_content_dot_product_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine Top 100 of each query by using Cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take ranking of cosine simalrity on title because it performed best\n",
    "final_merged_df = final_merged_df.sort_values(by=['id', 'title_cosine_sim'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9.9998e-01, 2.3085e-05]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained tokenizer and model\n",
    "model_name = \"amberoad/bert-multilingual-passage-reranking-msmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "query = \"What is the capital of France?\"\n",
    "document = \"I love me some baguette and cities\"\n",
    "\n",
    "# Tokenize the input (combining query and document)\n",
    "inputs = tokenizer(query, document, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Forward pass to get the logits (score)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits    \n",
    "\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4886,  3.3367]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168def182482475f93f55f1214aba1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Multilingual Cross encoder from https://huggingface.co/amberoad/bert-multilingual-passage-reranking-msmarco\n",
    "model_name = \"amberoad/bert-multilingual-passage-reranking-msmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "updated_rows = []\n",
    "\n",
    "# Iterate over each unique query_id\n",
    "for query_id in tqdm(final_merged_df['id'].unique()):\n",
    "    query_df = final_merged_df[final_merged_df['id'] == query_id]\n",
    "    \n",
    "    # Get top 100 rows based on title_cosine_sim\n",
    "    top_100_df = query_df.nlargest(100, 'title_cosine_sim')\n",
    "    remaining_df = query_df.drop(top_100_df.index)\n",
    "    \n",
    "    # Calculate cross_encoder scores for the top 100 rows\n",
    "    query_titles = top_100_df['query_title_translated'].tolist()\n",
    "    document_titles = top_100_df['document_title'].tolist()\n",
    "\n",
    "    # Tokenize all query-document pairs via batch processing\n",
    "    inputs = tokenizer(query_titles, document_titles, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Perform inference via batch processing\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract logits and apply softmax to get probabilities\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get relevance score - corresponds to the second value in the tensor\n",
    "    cross_encoder_scores = probs[:, 1].cpu().numpy()\n",
    "\n",
    "    # Update title_cosine_sim with cross_encoder scores for top 100 rows\n",
    "    updated_top_100_df = top_100_df.copy()\n",
    "    updated_top_100_df['new_title_cosine_sim'] = updated_top_100_df['title_cosine_sim'] + 0.1 * np.array(cross_encoder_scores)\n",
    "    \n",
    "    # Add updated rows to list\n",
    "    updated_rows.append(updated_top_100_df)\n",
    "    \n",
    "    # Keep rows outside top 100 unchanged\n",
    "    remaining_df['new_title_cosine_sim'] = remaining_df['title_cosine_sim']  \n",
    "    updated_rows.append(remaining_df)\n",
    "\n",
    "# Concatenate updated rows back into a DataFrame\n",
    "final_updated_df = pd.concat(updated_rows)\n",
    "\n",
    "# Sort DataFrame first by 'query_id' and then by 'new_title_cosine_sim' \n",
    "final_updated_df = final_updated_df.sort_values(by=['id', 'new_title_cosine_sim'], ascending=[True, False])\n",
    "\n",
    "# Update original 'title_cosine_sim' column with the new value\n",
    "final_updated_df['title_cosine_sim'] = final_updated_df['new_title_cosine_sim']\n",
    "\n",
    "# Drop unnecessary column\n",
    "final_updated_df = final_updated_df.drop(columns=['new_title_cosine_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{R@1000: 0.9289491760379356,\n",
       " AP: 0.26924577283993967,\n",
       " nDCG@20: 0.3057825092714263,\n",
       " RBP(rel=1): 0.42070581734197193,\n",
       " R@100: 0.19239881908238096}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract qrel data from dataframe and transform into list of tuples\n",
    "cross_encoder_runs = [(row.id, row.document_id, row.title_cosine_sim) for row in final_updated_df.itertuples(index=False)]\n",
    "# Evaluate cross-encoder scores \n",
    "evaluate(qrels, cross_encoder_runs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
