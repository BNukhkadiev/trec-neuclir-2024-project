{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM-Roberta EN RU\n",
    "Done by: Baga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "documents = pd.read_csv('data/documents_subset.csv', sep='\\t')\n",
    "documents['doc_title'] = documents['doc_title'].replace(r'[^\\w\\s]',' ',regex=True).replace(r'\\s+',' ',regex=True).str.lower()\n",
    "queries = pd.read_csv('data/queries.csv', sep='\\t')\n",
    "queries['query_text_rus'] = queries['query_text_rus'].replace(r'[^\\w\\s]',' ',regex=True).replace(r'\\s+',' ',regex=True).str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"DeepPavlov/xlm-roberta-large-en-ru\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper function to compute sentence embeddings\n",
    "def get_sentence_embedding(text, tokenizer, model):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    # Mean pooling to get sentence embedding\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    masked_embeddings = embeddings * mask\n",
    "    sentence_embedding = masked_embeddings.sum(dim=1) / mask.sum(dim=1)\n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "# Compute embeddings for queries\n",
    "query_embeddings = {}\n",
    "for _, query in queries.iterrows():\n",
    "    query_id = query[\"query_id\"]\n",
    "    query_text = query[\"query_text_rus\"]\n",
    "    query_embeddings[query_id] = get_sentence_embedding(query_text, tokenizer, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for documents\n",
    "document_embeddings = {}\n",
    "for _, document in documents.iterrows():\n",
    "    doc_id = document[\"doc_id\"]\n",
    "    doc_text = str(document[\"doc_title\"])\n",
    "    document_embeddings[doc_id] = get_sentence_embedding(doc_text, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to data/doc-embeddings-xlmr.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "# Save the dictionary to a pickle file\n",
    "pickle_file_path = \"data/doc-embeddings-xlmr.pkl\"\n",
    "with open(pickle_file_path, \"wb\") as f:\n",
    "    pickle.dump(document_embeddings, f)\n",
    "\n",
    "print(f\"Dictionary saved to {pickle_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"data/doc-embeddings-xlmr.pkl\"\n",
    "\n",
    "# Open the file in read-binary mode and load the dictionary\n",
    "with open(file_path, \"rb\") as file:\n",
    "    document_embeddings = pickle.load(file)\n",
    "\n",
    "# Now `my_dict` contains the dictionary loaded from the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rankings\n",
    "rankings = {}\n",
    "for query_id, query_embedding in query_embeddings.items():\n",
    "    scores = []\n",
    "    for doc_id, doc_embedding in document_embeddings.items():\n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(query_embedding.numpy(), doc_embedding.numpy())[0][0]\n",
    "        scores.append((doc_id, similarity))\n",
    "    # Sort documents by similarity in descending order\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    rankings[query_id] = scores\n",
    "\n",
    "\n",
    "# # Print rankings\n",
    "# for query_id, ranked_docs in rankings.items():\n",
    "#     print(f\"Query ID {query_id}:\")\n",
    "#     for doc_id, score in ranked_docs:\n",
    "#         print(f\"  Document ID: {doc_id}, Similarity Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean rankings:\n",
    "import numpy as np\n",
    "\n",
    "rankings = {}\n",
    "for query_id, query_embedding in query_embeddings.items():\n",
    "    scores = []\n",
    "    for doc_id, doc_embedding in document_embeddings.items():\n",
    "        # Compute Euclidean distance\n",
    "        distance = np.linalg.norm(query_embedding.numpy() - doc_embedding.numpy())\n",
    "        # Note: Smaller distance means higher similarity\n",
    "        scores.append((doc_id, -distance))  # Negate distance to sort in descending order\n",
    "    # Sort documents by distance (negated) in descending order\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    rankings[query_id] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minkowski distance\n",
    "# Define a parameter `p` for Minkowski distance\n",
    "p = 3  # Example: p = 3 for cubic distance; change as needed\n",
    "\n",
    "rankings = {}\n",
    "for query_id, query_embedding in query_embeddings.items():\n",
    "    scores = []\n",
    "    for doc_id, doc_embedding in document_embeddings.items():\n",
    "        # Compute Minkowski distance\n",
    "        distance = np.sum(np.abs(query_embedding.numpy() - doc_embedding.numpy()) ** p) ** (1 / p)\n",
    "        # Note: Smaller distance means higher similarity\n",
    "        scores.append((doc_id, -distance))  # Negate distance to sort in descending order\n",
    "    # Sort documents by distance (negated) in descending order\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    rankings[query_id] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>b76bf77f-78d4-460b-8d4e-1837bea7f6ef</td>\n",
       "      <td>-0.563927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>fce6a923-8a9d-441e-b565-f79c53dc1baf</td>\n",
       "      <td>-0.576330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>6259ed1e-02d3-438d-b3eb-950d51d95132</td>\n",
       "      <td>-0.600582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>1a49ac53-27fa-45bc-abda-f33927daaf28</td>\n",
       "      <td>-0.606003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>79228ae6-e047-4e1b-bd67-ed6873553834</td>\n",
       "      <td>-0.610377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>a9aa71e8-3049-4017-8523-65490482afa0</td>\n",
       "      <td>-0.612731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200</td>\n",
       "      <td>2fb2cba8-5d76-4dc7-8c2e-80dbd2870eb8</td>\n",
       "      <td>-0.614481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>e3fb40d6-e26c-4af9-a5c2-e019d64aed81</td>\n",
       "      <td>-0.614579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200</td>\n",
       "      <td>18afa815-690c-4c2c-be68-f0384c5ef8d3</td>\n",
       "      <td>-0.618613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200</td>\n",
       "      <td>29fdda1d-adea-44d7-a9dc-16124192c9c0</td>\n",
       "      <td>-0.620795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                doc_id     score\n",
       "0      200  b76bf77f-78d4-460b-8d4e-1837bea7f6ef -0.563927\n",
       "1      200  fce6a923-8a9d-441e-b565-f79c53dc1baf -0.576330\n",
       "2      200  6259ed1e-02d3-438d-b3eb-950d51d95132 -0.600582\n",
       "3      200  1a49ac53-27fa-45bc-abda-f33927daaf28 -0.606003\n",
       "4      200  79228ae6-e047-4e1b-bd67-ed6873553834 -0.610377\n",
       "5      200  a9aa71e8-3049-4017-8523-65490482afa0 -0.612731\n",
       "6      200  2fb2cba8-5d76-4dc7-8c2e-80dbd2870eb8 -0.614481\n",
       "7      200  e3fb40d6-e26c-4af9-a5c2-e019d64aed81 -0.614579\n",
       "8      200  18afa815-690c-4c2c-be68-f0384c5ef8d3 -0.618613\n",
       "9      200  29fdda1d-adea-44d7-a9dc-16124192c9c0 -0.620795"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten rankings into a DataFrame\n",
    "flattened_rankings = []\n",
    "for query_id, docs in rankings.items():\n",
    "    for doc_id, score in docs:\n",
    "        flattened_rankings.append({'query_id': str(query_id), 'doc_id': doc_id, 'score': score})\n",
    "\n",
    "# Convert to a DataFrame\n",
    "flattened_rankings_df = pd.DataFrame(flattened_rankings)\n",
    "# Ensure documents are sorted by score for each query\n",
    "flattened_rankings_df = flattened_rankings_df.sort_values(by=['query_id', 'score'], ascending=[True, False])\n",
    "\n",
    "# Display the flattened rankings\n",
    "flattened_rankings_df[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import nDCG, MAP, RBP, Recall, Qrel, ScoredDoc\n",
    "\n",
    "\n",
    "qrels_pd = pd.read_csv('data/qrels.csv', sep='\\t')\n",
    "\n",
    "qrels = [\n",
    "    ir_measures.Qrel(query_id=str(row['query_id']), doc_id=row['doc_id'], relevance=row['relevance_class'])\n",
    "    for _, row in qrels_pd.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(qrels, result):\n",
    "    runs = [\n",
    "        ScoredDoc(query_id=row['query_id'], doc_id=row['doc_id'], score=row['score'])\n",
    "        for _, row in result.iterrows()\n",
    "    ]\n",
    "\n",
    "    metrics = [\n",
    "        ir_measures.nDCG @ 20,   # nDCG@20\n",
    "        ir_measures.AP,          # Average Precision\n",
    "        ir_measures.RBP(rel=1),  # Relevance Based Precision\n",
    "        ir_measures.R @ 100,     # Recall@100\n",
    "        ir_measures.R @ 1000     # Recall@1000\n",
    "    ]\n",
    "\n",
    "    scores = ir_measures.calc_aggregate([nDCG@20, MAP, RBP(rel=1), Recall@100, Recall@1000], qrels, runs)\n",
    "    # scores = ir_measures.calc_aggregate([nDCG@20, MAP, Recall@100, Recall@1000], qrels, runs)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics ROBERTA:\n",
      "R@100: 0.11560332034582407\n",
      "RBP(rel=1): 0.173437160259603\n",
      "nDCG@20: 0.13043452326804456\n",
      "R@1000: 0.22229372723690713\n",
      "AP: 0.04244673716818182\n"
     ]
    }
   ],
   "source": [
    "performance_tfidf = evaluate(qrels, flattened_rankings_df)\n",
    "print(\"Evaluation Metrics ROBERTA:\")\n",
    "for metric, value in performance_tfidf.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
